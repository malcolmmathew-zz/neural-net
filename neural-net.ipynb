{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Neural Networks\n",
    "\n",
    "The purpose of this notebook is to create an implementation of a simple  artificial neural network for learning purposes. \n",
    "\n",
    "Here are the main concepts associated with an ANN:\n",
    "- Activation Function \n",
    "- Forward Propagation\n",
    "- Gradient Descent\n",
    "- Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(([3,5, 8],[5,1, 10]), dtype=float)\n",
    "y = np.array(([75],[82]), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X = X/np.amax(X, axis=0)\n",
    "y = y/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6  1.   0.8]\n",
      "[[ 0.75]\n",
      " [ 0.82]]\n"
     ]
    }
   ],
   "source": [
    "print X[0,:]\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    def __init__(self):\n",
    "        self.num_layers = 3\n",
    "        self.input_layer_size = 3\n",
    "        self.output_layer_size = 1\n",
    "        self.hidden_layer_size  = 3\n",
    "\n",
    "        self.w1 = np.random.randn(self.hidden_layer_size, self.input_layer_size)    \n",
    "        self.w2 = np.random.randn(self.output_layer_size, self.hidden_layer_size)\n",
    "        self.w = {1: self.w1, 2: self.w2}\n",
    "        \n",
    "        self.b1 = np.random.randn(self.hidden_layer_size, 1)\n",
    "        self.b2 = np.random.randn(self.output_layer_size, 1)\n",
    "        self.b = {1: self.b1, 2: self.b2}\n",
    "        \n",
    "        self.z = {}\n",
    "        self.a = {}\n",
    "        self.delta = {}\n",
    "        \n",
    "        self.deltaW = {}\n",
    "        self.deltab = {}\n",
    "            \n",
    "    def forward_propagation(self, X):\n",
    "        for l in range(1, self.num_layers, 1):\n",
    "            print l\n",
    "            if l == 1:\n",
    "                node_in = X       \n",
    "                self.a[l] = X\n",
    "            else:\n",
    "                node_in = self.a[l]\n",
    "            self.z[l+1] = np.dot(self.w[l], node_in) + self.b[l]  \n",
    "            self.a[l+1] = self.sigmoid(self.z[l+1])\n",
    "    \n",
    "#     def backpropagation(self, X, Y, z):\n",
    "#         y = self.forward_propagation(X)\n",
    "#         self.delta[self.num_layers] = np.multiply(-(Y - y), \\\n",
    "#                                    self.sigmoid_derivative(self.z[self.num_layers]))\n",
    "              \n",
    "        \n",
    "#         for l in range(self.num_layers - 1, 1, -1):\n",
    "#             self.delta[l] = np.multiply(np.dot(self.w[l].T, delta[l+1]), self.sigmoid_derivative())            \n",
    "        \n",
    "#         return np.dot(delta[z+1], activations[z].T), delta[z+1]\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return np.exp(-z)/(1+np.exp(-z)**2)\n",
    "    \n",
    "    def delta_outer_layer(self, y, a, z):\n",
    "        return np.multiply(-(y-a), self.sigmoid_derivative(z))\n",
    "    \n",
    "    def delta_hidden_layer(self, delta_next_layer, w, z):\n",
    "        return np.multiply(np.dot(np.transpose(w), delta_next_layer), self.sigmoid_derivative(z))  \n",
    "#         return np.dot(np.transpose(w), delta_next_layer) * self.sigmoid_derivative(z)\n",
    "    \n",
    "    def train_model(self, X, y, iter_num=3000, alpha=0.25):\n",
    "        m = len(y)\n",
    "        average_cost_function = []\n",
    "        \n",
    "        for r in range(iter_num):\n",
    "            self.initialize_delta_values()\n",
    "            average_cost = 0\n",
    "            \n",
    "            for i in range(m):\n",
    "                self.forward_propagation(X[i, :])\n",
    "\n",
    "                for l in range(self.num_layers, 0, -1):\n",
    "                    if l == self.num_layers:\n",
    "                        self.delta[l] = self.delta_outer_layer(y[i,:], \\\n",
    "                                                     self.a[l], \\\n",
    "                                                     self.z[l])\n",
    "                        average_cost += np.linalg.norm((y[i,:]-self.a[l]))\n",
    "#                         print self.delta[l]\n",
    "                    else:\n",
    "                        if l>1:\n",
    "                            self.delta[l] = self.delta_hidden_layer(self.delta[l+1], \\\n",
    "                                                                    self.w[l], \\\n",
    "                                                                    self.z[l])\n",
    "                        print \"DA ONE\"\n",
    "                        print self.a[l]\n",
    "                        print self.delta[l+1][:,np.newaxis]\n",
    "                        \n",
    "                        print \"delta of \" + str(l+1)\n",
    "                        print self.delta[l+1].shape\n",
    "                        print np.transpose(self.a[l]).shape\n",
    "                        self.deltaW[l] += np.dot(self.delta[l+1][:,np.newaxis], \\\n",
    "                                              np.transpose(self.a[l][:,np.newaxis]))\n",
    "#                                               np.transpose(self.a[l][:,np.newaxis]))\n",
    "                        print self.deltaW\n",
    "                        self.deltab[l] += self.delta[l+1]\n",
    "                        \n",
    "            for l in range(self.num_layers - 1, 0, -1):\n",
    "                self.w = -alpha * (float(1)/m * self.deltaW[l])\n",
    "                self.b = -alpha * (float(1)/m * self.deltab[l])\n",
    "            \n",
    "            average_cost = float(1)/m * average_cost\n",
    "            average_cost_function.append(average_cost)\n",
    "        \n",
    "        return self.w, self.b, average_cost_function\n",
    "    \n",
    "    def initialize_delta_values(self):\n",
    "        \n",
    "        w1 = np.zeros((self.hidden_layer_size, self.input_layer_size))   \n",
    "        w2 = np.zeros((self.output_layer_size, self.hidden_layer_size))\n",
    "        self.deltaW = {1: w1, 2: w2}\n",
    "        \n",
    "        b1 = np.zeros((self.hidden_layer_size, 1))\n",
    "        b2 = np.zeros((self.output_layer_size, 1))\n",
    "        self.deltab = {1: b1, 2: b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "DA ONE\n",
      "[[ 0.63453336  0.85642343  0.82292244]\n",
      " [ 0.22207581  0.49514298  0.43314009]\n",
      " [ 0.21811019  0.48936864  0.42747658]]\n",
      "[[[-0.15441383 -0.16697368 -0.16639273]]]\n",
      "delta of 3\n",
      "(1, 3)\n",
      "(3, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,1,3) and (3,1,3) not aligned: 3 (dim 2) != 1 (dim 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-56c1a2d54522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-ff0fe7614979>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, X, y, iter_num, alpha)\u001b[0m\n\u001b[1;32m     82\u001b[0m                         \u001b[0;32mprint\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                         \u001b[0;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeltaW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                               \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;31m#                                               np.transpose(self.a[l][:,np.newaxis]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                         \u001b[0;32mprint\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeltaW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,1,3) and (3,1,3) not aligned: 3 (dim 2) != 1 (dim 1)"
     ]
    }
   ],
   "source": [
    "NN = NeuralNet()\n",
    "NN.train_model(X,y)\n",
    "\n",
    "# print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "There are a few important parts to building out a basic Artificial Neural Network. The first thing to do is define the concept of an activation/sigmoid function.\n",
    "\n",
    "$$f(z)= \\frac{1}{1+\\exp(-x)}$$\n",
    "\n",
    "This is the function applied to the inputs of all nodes in the network  and its result is passed to the next layer as an input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def sigmoid(self, z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "This activation function has a weight assigned to its input for each individual connection from one node to another. The sum of these results become the output to feed into the next layer of nodes. This concept is called forward propagation.\n",
    "\n",
    "Implementation-wise, we use numpy arrays and matrix operations to make this function simpler and more time-efficient. In addition, we include an extra bias node in each layer of the network except the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def forward_propagation(self, X):\n",
    "    for l in range(self.num_layers-1):\n",
    "        if l == 0:\n",
    "            bias_values = np.array([1 for r in range(X.shape[0])]).reshape(X.shape[0], 1)\n",
    "            node_in = np.concatenate((bias_values, X), axis=1)\n",
    "            node_in = node_in.reshape(node_in.shape[1], node_in.shape[0])\n",
    "        else:\n",
    "            bias_values = np.array([1 for r in range(h.shape[1])]).reshape(1, h.shape[1])\n",
    "            node_in = np.concatenate((h, bias_values), axis=0)\n",
    "\n",
    "        z = np.dot(self.w[l], node_in)            \n",
    "        h = self.sigmoid(z)     \n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient Descent and Cost Function\n",
    "\n",
    "We now have a function that will take an input based on initial arbitrarily generated weights. This will produce a value that makes a prediction of what the data should be. However, this is bound to be off by a certain measure. This difference between a predicted and actual value is a cost, which we will strictly define through a cost function.\n",
    "\n",
    "$$J(w) = \\frac{1}{m}\\sum \\frac{1}{2} (y^{z} - h^{n_{l}}(x^{z}))^{2} $$\n",
    "\n",
    "Here, m represents the number of training samples and $h^{n_{l}}$ is the output of the final activation layer.\n",
    "\n",
    "We can attempt to minimize this cost function iteratively by calculating its derivative and using that gradient to iteratively move through the function to find its minimum value, aka the best predictor of the actual data. We are specifically manipulating each weight in the network to achieve this, shown in the computation below:\n",
    "\n",
    "$$w^{(l)}_{ij} = w^{(l)}_{ij} - \\alpha \\frac{\\partial}{\\partial  w^{(l)}_{ij}}J(w)$$\n",
    "\n",
    "Here, $\\alpha$ represents the step size that indicates the magnitude of the change in weight. It will determine the speed at which gradient descent converges to a solution. To stop gradient descent, we will need to define how accurate (how small the error) the model should be. \n",
    "\n",
    "Also, i and j refers to the nodes that the weights are associated with, where i is the destination node and j is the source. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll delve into some math now to show how a partial derivative for a given $w_{ij}$ is cancelled. Let's look at $\\frac{\\partial J}{\\partial  w^{(2)}_{12}}$. We can separate this into a chain of derivatives.\n",
    "\n",
    "Let's make some definitions first. we will say the output layer function looks like the following:\n",
    "\n",
    "$$h_{1}^{3} = f(w_{11}^{2}h_{1}^{2} + w_{12}^{2}h_{2}^{2} + w_{13}^{2}h_{3}^{2}) = f(z_{1}^{(2)})$$\n",
    "\n",
    "So, we can can define our derivative as:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w^{(2)}_{12}} = \\frac{\\partial J}{\\partial h^{(3)}_{1}} \\frac{\\partial h^{(3)}_{1}}{\\partial z_{1}^{2}} \\frac{\\partial z_{1}^{2}}{\\partial w^{(2)}_{12}} $$\n",
    "\n",
    "We will now evaluate each one separately. We can simplify the third term to:\n",
    "\n",
    "$$\\frac{\\partial z_{1}^{2}}{\\partial w^{(2)}_{12}} = h_{2}^{2}$$\n",
    "\n",
    "For the second term, we simply require the derivative of the activation function defined earlier:\n",
    "\n",
    "$$\\frac{\\partial h}{\\partial z} = f(z)(1 - f(z))$$\n",
    "\n",
    "The final term is the derivative of the cost function with respect to the output of the activation function. This result is:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial h} = -(y_{1} - h_{1}^{(3)})$$\n",
    "\n",
    "So, the product of thee three results will give us the derivative for that particular weight. \n",
    "\n",
    "To generalize this result, let's define a new variable $\\delta$:\n",
    "\n",
    "$$\\delta^{(n_{l})}_{i} = -(y_{i}-h_{i}^{(n_{l})})f'(z_{i}^{(n_{l})})$$\n",
    "\n",
    "Extrapolating for any given connection ij, we can see that:\n",
    "\n",
    "$$\\frac {\\partial J(W)}{\\partial W_{ij}^{(l)}} = h_{j}^{l} \n",
    "\\delta_{i}^{(l+1)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "The above derivations work well for the weights that are closest to the output layer. We need a better way to update the weights in deeper layers. We achieve this with the backpropagation method. \n",
    "\n",
    "We need to propagate the $\\delta^{(n_{l})}_{i}$ to previous layers. The delta function of nodes from previous layers will be the delta function of the next layer multiplied by the connecting weight from the source to destination node. \n",
    "\n",
    "$$\\delta^{(l)}_{j} = \\delta^{(l=1)}_{1}w^{(l)}_{1j}f'(z_{j})^{(l)}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backpropagation():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN  = Neural_Net()\n",
    "\n",
    "c1 = NN.cost_function_derivative(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " a,b = NN.cost_function_derivative(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06366729,  0.02766799,  0.25473514],\n",
       "       [-0.04287048,  0.01929333,  0.18918441]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2191875 ],\n",
       "       [-0.32555071],\n",
       "       [-0.19873838]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$c = \\sqrt{a^2 + b^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
